{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark + Scala + Cassandra examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports! The datastax jars are baked into my Dockerfile.notebook file, this is a slight pain, you need the scala version to match with those on the spark workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.math.random                                                                   \n",
    "import org.apache.spark.SparkContext                                                       \n",
    "import org.apache.spark.SparkContext._                                                     \n",
    "import org.apache.spark.SparkConf                                                          \n",
    "import org.apache.spark.sql.SparkSession                                                   \n",
    "import com.datastax.spark.connector._                                                      \n",
    "import org.apache.spark.sql.cassandra._ \n",
    "import com.datastax.spark.connector.cql.CassandraConnectorConf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the hostname by using 'kubectl get pods -o wide', the pod ip will be the one to select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val hostname = \"100.96.2.8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a sparkcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val conf = new SparkConf().setAppName(\"SparkApp\")\n",
    "conf.set(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "conf.set(\"spark.cassandra.connection.host\", hostname)\n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create some key/value pairs and insert them into cassandra if test.kv exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 0) / 2]"
     ]
    }
   ],
   "source": [
    "import java.time.Instant\n",
    "val ts = Instant.now.getEpochSecond\n",
    "val collection = sc.parallelize(Seq((\"key3\", ts, 3), (\"key4\", ts, 4))) \n",
    "collection.saveToCassandra(\"dev\", \"measures\", SomeColumns(\"station_id\", \"time\", \"value\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199                                                                             \n"
     ]
    }
   ],
   "source": [
    "var rdd = sc.cassandraTable(\"dev\", \"measures\")           \n",
    "println(rdd.map(_.getInt(\"value\")).max())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@25d764e9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder.appName(\"Simple Application\").getOrCreate()\n",
    "spark.setCassandraConf(\"cluster\", CassandraConnectorConf.ConnectionHostParam.option(hostname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This displays the table if it exists (max 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@664e3b77 [station_id#8,time#9,value#10] ReadSchema: struct<station_id:string,time:timestamp,value:decimal(38,18)>\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- value: decimal(38,18) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(Map(                                                                          \n",
    "       \"table\" -> \"measures\",                                                               \n",
    "       \"keyspace\" -> \"dev\",\n",
    "       \"cluster\" -> \"cluster\")).load() \n",
    "df.explain()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val createDDL = \"\"\"CREATE TEMPORARY VIEW words\n",
    "     USING org.apache.spark.sql.cassandra\n",
    "     OPTIONS (\n",
    "     table \"measures\",\n",
    "     keyspace \"dev\",\n",
    "     cluster \"cluster\",\n",
    "     pushdown \"true\")\"\"\"\n",
    "spark.sql(createDDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+\n",
      "|station_id|               time|               value|\n",
      "+----------+-------------------+--------------------+\n",
      "|  station1|1960-02-16 09:00:00|81.10724639892578...|\n",
      "|  station1|1960-02-16 10:00:00|190.1847534179687...|\n",
      "|  station1|1960-02-16 11:00:00|97.56996917724609...|\n",
      "|  station1|1960-02-16 12:00:00|124.5751495361328...|\n",
      "|  station1|1960-02-16 13:00:00|53.32531738281250...|\n",
      "|  station1|1960-02-16 14:00:00|168.5605316162109...|\n",
      "|  station1|1960-02-16 15:00:00|146.4290618896484...|\n",
      "|  station1|1960-02-16 16:00:00|197.0472869873046...|\n",
      "|  station1|1960-02-16 17:00:00|160.8878173828125...|\n",
      "|  station1|1960-02-16 18:00:00|136.0335540771484...|\n",
      "|  station1|1960-02-16 19:00:00|50.38066101074218...|\n",
      "|  station1|1960-02-16 20:00:00|11.29835891723632...|\n",
      "|  station1|1960-02-16 21:00:00|23.29217147827148...|\n",
      "|  station1|1960-02-16 22:00:00|66.18018341064453...|\n",
      "|  station1|1960-02-16 23:00:00|136.6040191650390...|\n",
      "|  station1|1960-02-17 00:00:00|116.7664794921875...|\n",
      "|  station1|1960-02-17 01:00:00|155.7245178222656...|\n",
      "|  station1|1960-02-17 02:00:00|5.837512016296386719|\n",
      "|  station1|1960-02-17 03:00:00|7.623910903930664063|\n",
      "|  station1|1960-02-17 04:00:00|12.70929527282714...|\n",
      "+----------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------------------+\n",
      "|station_id|               value|\n",
      "+----------+--------------------+\n",
      "|  station2|7.518172264099121094|\n",
      "|  station2|175.8346252441406...|\n",
      "|  station2|115.1638946533203...|\n",
      "|  station2|158.8481903076171...|\n",
      "|  station2|176.7501678466796...|\n",
      "|  station2|13.35755538940429...|\n",
      "|  station2|73.61056518554687...|\n",
      "|  station2|144.5174407958984...|\n",
      "|  station2|170.8857727050781...|\n",
      "|  station2|32.36848068237304...|\n",
      "|  station2|44.35338973999023...|\n",
      "|  station2|184.4829711914062...|\n",
      "|  station2|19.89500427246093...|\n",
      "|  station2|167.9343414306640...|\n",
      "|  station2|19.56144523620605...|\n",
      "|  station2|187.0158538818359...|\n",
      "|  station2|26.10206604003906...|\n",
      "|  station2|125.8531799316406...|\n",
      "|  station2|161.8094329833984...|\n",
      "|  station2|67.19683074951171...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from words\").show()\n",
    "spark.sql(\"SELECT station_id, value from words where station_id='station2'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
