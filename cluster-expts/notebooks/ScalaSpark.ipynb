{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark + Scala + Cassandra examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports! The datastax jars are baked into my Dockerfile.notebook file, this is a slight pain, you need the scala version to match with those on the spark workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.math.random                                                                   \n",
    "import org.apache.spark.SparkContext                                                       \n",
    "import org.apache.spark.SparkContext._                                                     \n",
    "import org.apache.spark.SparkConf                                                          \n",
    "import org.apache.spark.sql.SparkSession                                                   \n",
    "import com.datastax.spark.connector._                                                      \n",
    "import org.apache.spark.sql.cassandra._ \n",
    "import com.datastax.spark.connector.cql.CassandraConnectorConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val hostname = \"cassandra\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubernetes/docker-compose are useful here, \"cassandra\" is the hostname of the cassandra-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val conf = new SparkConf().setAppName(\"SparkApp\")\n",
    "conf.set(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "conf.set(\"spark.cassandra.connection.host\", hostname)\n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create some key/value pairs and insert them into cassandra if test.kv exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.io.IOException\n",
       "Message: Couldn't find test.kv or any similarly named keyspace and table pairs\n",
       "StackTrace:   at com.datastax.spark.connector.cql.Schema$.tableFromCassandra(Schema.scala:356)\n",
       "  at com.datastax.spark.connector.writer.TableWriter$.apply(TableWriter.scala:344)\n",
       "  at com.datastax.spark.connector.RDDFunctions.saveToCassandra(RDDFunctions.scala:35)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val collection = sc.parallelize(Seq((\"key3\", 3), (\"key4\", 4))) \n",
    "var rdd = sc.cassandraTable(\"test\", \"kv\")                                                          \n",
    "collection.saveToCassandra(\"test\", \"kv\", SomeColumns(\"key\", \"value\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@df698a7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder.appName(\"Simple Application\").getOrCreate()\n",
    "spark.setCassandraConf(\"cluster\", CassandraConnectorConf.ConnectionHostParam.option(hostname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This displays the table if it exists (max 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.io.IOException\n",
       "Message: Couldn't find test.kv or any similarly named keyspace and table pairs\n",
       "StackTrace:   at com.datastax.spark.connector.cql.Schema$.tableFromCassandra(Schema.scala:356)\n",
       "  at org.apache.spark.sql.cassandra.CassandraSourceRelation.<init>(CassandraSourceRelation.scala:46)\n",
       "  at org.apache.spark.sql.cassandra.CassandraSourceRelation$.apply(CassandraSourceRelation.scala:287)\n",
       "  at org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:56)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:306)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(Map(                                                                          \n",
    "       \"table\" -> \"kv\",                                                               \n",
    "       \"keyspace\" -> \"test\",\n",
    "       \"cluster\" -> \"cluster\")).load()                                                         \n",
    "      df.show()                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
